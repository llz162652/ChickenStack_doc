"""
ğŸ” ChickenStack è§£æå™¨æ€§èƒ½åŸºå‡†æµ‹è¯•
====================================

æœ¬è„šæœ¬ç”¨äºæµ‹è¯•è§£æå™¨çš„æ€§èƒ½ï¼Œå¯¹æ¯”ä¼˜åŒ–å‰åçš„è§£æé€Ÿåº¦ã€‚

## è¿è¡Œæ–¹æ³•

```bash
python tests/benchmark_parser.py
```

## æµ‹è¯•ç”¨ä¾‹

1. ç®€å•æ•°å­¦è¿ç®—
2. å°å¾ªç¯
3. ä¸­ç­‰å¾ªç¯
4. å¤§å¾ªç¯
5. å­—ç¬¦ä¸²æ‰“å°
"""

import time
import sys
import os
import json
from datetime import datetime

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from chicken_stack import Parser


def ensure_results_dir():
    """ç¡®ä¿æµ‹è¯•ç»“æœç›®å½•å­˜åœ¨"""
    results_dir = os.path.join(os.path.dirname(__file__), 'results')
    if not os.path.exists(results_dir):
        os.makedirs(results_dir)
    return results_dir


def benchmark_parser():
    """è¿è¡Œè§£æå™¨æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    parser = Parser()

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        ("ç®€å•æ•°å­¦è¿ç®—", "10 20 + ."),
        ("å°å¾ªç¯", "5 [ : . 1 - ]"),
        ("ä¸­ç­‰å¾ªç¯", "100 [ : . 1 - ]"),
        ("å¤§å¾ªç¯", "1000 [ 1 + ]"),
        ("å­—ç¬¦ä¸²æ‰“å°", '72 " 101 " 108 " 108 " 111 " 32 " 87 " 111 " 114 " 108 " 100 " 33 " 10 "'),
    ]

    # è¿è¡Œ20æ¬¡æµ‹è¯•
    num_runs = 20
    all_results = []

    print("ChickenStack è§£æå™¨æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("=" * 90)
    print(f"æµ‹è¯•æ¬¡æ•°: {num_runs}")
    print("=" * 90)

    for run in range(num_runs):
        print(f"\n[ç¬¬ {run + 1}/{num_runs} æ¬¡æµ‹è¯•]")
        run_results = {}

        for name, code in test_cases:
            # é¢„çƒ­
            for _ in range(10):
                parser.parse(code)
                parser.reset()

            # æµ‹è¯•
            iterations = 10000
            start = time.perf_counter()
            for _ in range(iterations):
                parser.parse(code)
                parser.reset()
            elapsed = time.perf_counter() - start

            avg_time = elapsed / iterations * 1000  # æ¯«ç§’
            run_results[name] = {
                'avg_time_ms': avg_time,
                'total_time_s': elapsed
            }

        all_results.append(run_results)

    # è®¡ç®—å¹³å‡å€¼
    print("\n" + "=" * 90)
    print("[å¹³å‡ç»“æœ]")
    print("=" * 90)
    print(f"{'æµ‹è¯•ç”¨ä¾‹':<30} {'å¹³å‡æ—¶é—´ (ms)':<20} {'æœ€å°å€¼ (ms)':<20} {'æœ€å¤§å€¼ (ms)':<20}")
    print("=" * 90)

    summary = {}
    for name, code in test_cases:
        times = [run[name]['avg_time_ms'] for run in all_results]
        avg = sum(times) / len(times)
        min_val = min(times)
        max_val = max(times)

        summary[name] = {
            'average_ms': avg,
            'min_ms': min_val,
            'max_ms': max_val,
            'all_values': times
        }

        print(f"{name:<30} {avg:<20.6f} {min_val:<20.6f} {max_val:<20.6f}")

    print("=" * 90)

    # è®¡ç®—æ€»å¹³å‡æ—¶é—´
    total_avg = sum(r['average_ms'] for r in summary.values())
    print(f"{'æ€»å¹³å‡æ—¶é—´':<30} {total_avg:<20.6f}")
    print("=" * 90)

    # æ€§èƒ½è¯„ä¼°
    print("\n[æ€§èƒ½è¯„ä¼°]")
    print("-" * 90)
    if total_avg < 1.0:
        print("[EXCELLENT] æ€§èƒ½ä¼˜ç§€ï¼è§£æé€Ÿåº¦éå¸¸å¿«ã€‚")
    elif total_avg < 5.0:
        print("[GOOD] æ€§èƒ½è‰¯å¥½ï¼è§£æé€Ÿåº¦ç¬¦åˆé¢„æœŸã€‚")
    elif total_avg < 10.0:
        print("[AVERAGE] æ€§èƒ½ä¸€èˆ¬ã€‚å¯ä»¥è€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")
    else:
        print("[POOR] æ€§èƒ½è¾ƒå·®ã€‚å»ºè®®è¿›è¡Œæ€§èƒ½ä¼˜åŒ–ã€‚")
    print("-" * 90)

    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    results_dir = ensure_results_dir()
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"benchmark_{timestamp}.json"
    filepath = os.path.join(results_dir, filename)

    result_data = {
        'timestamp': timestamp,
        'num_runs': num_runs,
        'summary': summary,
        'all_runs': all_results,
        'total_average_ms': total_avg
    }

    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(result_data, f, indent=2, ensure_ascii=False)

    print(f"\n[ç»“æœå·²ä¿å­˜] {filepath}")


if __name__ == "__main__":
    benchmark_parser()